{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis on IMDB dataset\n",
    "\n",
    "http://nlpprogress.com/english/sentiment_analysis.html\n",
    "\n",
    "Top: [XLNet (Yang et al., 2019)](https://arxiv.org/pdf/1906.08237.pdf), accuracy: 96.21\n",
    "\n",
    "But can we get near that with a \"Simple but tough to beat ..\" encoder?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/talmago/git/simple-but-tough-to-beat-examples/.venv/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from encoder import build_from_fasttext_bin\n",
    "from nn import train_w2v, train_nn, fasttext, load_model\n",
    "from utils import read_imdb, preprocess_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download [IMDB dataset](https://ai.stanford.edu/~ang/papers/acl11-WordVectorsSentimentAnalysis.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = read_imdb(subset='train')\n",
    "X_test, y_test = read_imdb(subset='test')\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a corpus for word2vec training and pre-process with `textacy` lib:\n",
    "  - normalize unicode charset.\n",
    "  - deaccent (rÃ¨sume -> resume)\n",
    "  - unpack contractions (he's --> he is).\n",
    "  - remove emojis, hashtags, URLs, emails, etc\n",
    "  - remove punctuation marks\n",
    "  - strip whitespace\n",
    "  - lowercase\n",
    "\n",
    "train word2vec skipgram model as follows;\n",
    "  - dim = 200\n",
    "  - lr = relatively low.\n",
    "  - epochs = 15 (but should probably be ~ 25).\n",
    "  - ws = 5 (but should probably be ~ 7).\n",
    "  - sub-word information (minn = 3, maxn = 6).\n",
    "  \n",
    "alternatively, we can use a [pre-built model](https://fasttext.cc/docs/en/pretrained-vectors.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "W2V_PREBUILT_MODEL = 'cc.en.300.bin'\n",
    "W2V_MODEL = 'model.bin' # W2V_PREBUILT_MODEL\n",
    "\n",
    "if W2V_MODEL == W2V_PREBUILT_MODEL:\n",
    "    ! [[ ! -f {W2V_PREBUILT_MODEL} ]] && wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/{W2V_PREBUILT_MODEL}.gz\n",
    "    ! [[ ! -f {W2V_PREBUILT_MODEL} ]] && gzip -d {W2V_PREBUILT_MODEL}.gz\n",
    "    ! ls -lh {W2V_PREBUILT_MODEL}\n",
    "\n",
    "if not os.path.isfile(W2V_MODEL):\n",
    "    # build w2v corpus\n",
    "    corpus = []\n",
    "    raw_sentences, _ = read_imdb(subset=None, with_label=False)\n",
    "    for raw_sentence in tqdm(raw_sentences):\n",
    "        sent = preprocess_sentence(raw_sentence)\n",
    "        corpus.append(sent)\n",
    "\n",
    "    # train word2vec\n",
    "    model = train_w2v(corpus,\n",
    "                      model='skipgram',\n",
    "                      dim=200,\n",
    "                      min_count=20,\n",
    "                      lr=0.015,\n",
    "                      epoch=20,\n",
    "                      ws=7,\n",
    "                      minn=3,\n",
    "                      maxn=6)\n",
    "    # save model\n",
    "    model.save_model(W2V_MODEL)\n",
    "\n",
    "else: # load prebuilt model\n",
    "    model = fasttext.load_model(W2V_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec ---> \"Simple But Tough to Beat ..\" encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_encoder = build_from_fasttext_bin(model, preprocessor=preprocess_sentence, weighted=True)\n",
    "\n",
    "del model # free some memory !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape =  (25000, 200)\n"
     ]
    }
   ],
   "source": [
    "X_train = sentence_encoder.fit_transform(X_train)\n",
    "print('X_train.shape = ', X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test.shape =  (25000, 200)\n"
     ]
    }
   ],
   "source": [
    "X_test = sentence_encoder.transform(X_test)\n",
    "print('X_test.shape = ', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train a binary classification net:\n",
    "  - 1 hidden layer (128).\n",
    "  - dropout ~ [0.2 - 0.5].\n",
    "  - binary logloss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/20\n",
      "25000/25000 [==============================] - 2s 60us/sample - loss: 0.4296 - accuracy: 0.8337 - val_loss: 0.3037 - val_accuracy: 0.8811\n",
      "Epoch 2/20\n",
      "25000/25000 [==============================] - 1s 49us/sample - loss: 0.2942 - accuracy: 0.8814 - val_loss: 0.2778 - val_accuracy: 0.8885\n",
      "Epoch 3/20\n",
      "25000/25000 [==============================] - 1s 48us/sample - loss: 0.2795 - accuracy: 0.8865 - val_loss: 0.2722 - val_accuracy: 0.8897\n",
      "Epoch 4/20\n",
      "25000/25000 [==============================] - 1s 47us/sample - loss: 0.2746 - accuracy: 0.8882 - val_loss: 0.2666 - val_accuracy: 0.8938\n",
      "Epoch 5/20\n",
      "25000/25000 [==============================] - 1s 47us/sample - loss: 0.2731 - accuracy: 0.8907 - val_loss: 0.2658 - val_accuracy: 0.8939\n",
      "Epoch 6/20\n",
      "25000/25000 [==============================] - 1s 48us/sample - loss: 0.2701 - accuracy: 0.8904 - val_loss: 0.2670 - val_accuracy: 0.8918\n",
      "Epoch 7/20\n",
      "25000/25000 [==============================] - 1s 48us/sample - loss: 0.2678 - accuracy: 0.8912 - val_loss: 0.2652 - val_accuracy: 0.8922\n",
      "Epoch 8/20\n",
      "25000/25000 [==============================] - 1s 47us/sample - loss: 0.2681 - accuracy: 0.8910 - val_loss: 0.2644 - val_accuracy: 0.8930\n",
      "Epoch 9/20\n",
      "25000/25000 [==============================] - 1s 47us/sample - loss: 0.2665 - accuracy: 0.8927 - val_loss: 0.2640 - val_accuracy: 0.8932\n",
      "Epoch 10/20\n",
      "25000/25000 [==============================] - 1s 48us/sample - loss: 0.2651 - accuracy: 0.8934 - val_loss: 0.2645 - val_accuracy: 0.8936\n",
      "Epoch 11/20\n",
      "25000/25000 [==============================] - 1s 48us/sample - loss: 0.2657 - accuracy: 0.8932 - val_loss: 0.2635 - val_accuracy: 0.8932\n",
      "Epoch 12/20\n",
      "25000/25000 [==============================] - 1s 47us/sample - loss: 0.2641 - accuracy: 0.8943 - val_loss: 0.2652 - val_accuracy: 0.8926\n",
      "Epoch 13/20\n",
      "25000/25000 [==============================] - 1s 48us/sample - loss: 0.2630 - accuracy: 0.8940 - val_loss: 0.2639 - val_accuracy: 0.8938\n",
      "Epoch 14/20\n",
      "25000/25000 [==============================] - 1s 48us/sample - loss: 0.2624 - accuracy: 0.8928 - val_loss: 0.2647 - val_accuracy: 0.8931\n",
      "Epoch 15/20\n",
      "25000/25000 [==============================] - 1s 47us/sample - loss: 0.2616 - accuracy: 0.8946 - val_loss: 0.2637 - val_accuracy: 0.8932\n"
     ]
    }
   ],
   "source": [
    "MODEL_PT = 'model.h5'\n",
    "\n",
    "model = train_nn(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    hidden_layers=(128,),\n",
    "    activation='relu',\n",
    "    dropout=0.4,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_split=None,\n",
    "    validation_data=(X_test, y_test),\n",
    "    patience=4,\n",
    "    shuffle=True,\n",
    "    optimizer='adam',\n",
    "    pt=MODEL_PT,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "89% accuracy with a pretty simple encoder ! that's nice !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.89      0.90      0.89     12500\n",
      "         pos       0.89      0.89      0.89     12500\n",
      "\n",
      "    accuracy                           0.89     25000\n",
      "   macro avg       0.89      0.89      0.89     25000\n",
      "weighted avg       0.89      0.89      0.89     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = load_model(MODEL_PT)\n",
    "preds = model.predict_classes(X_test, batch_size=32)\n",
    "preds = preds.reshape(preds.shape[0])\n",
    "\n",
    "report = classification_report(y_test, preds, target_names=label_encoder.classes_)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
